{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd1eb51",
   "metadata": {},
   "source": [
    "# Robotic Process Automation: Web Scraper\n",
    "\n",
    "## Contacts:\n",
    "cursifrancesco@gmail.com\n",
    "\n",
    "## Description\n",
    "\n",
    "This projects describes how to make an automated web scraper to read text from a web page and download data.\n",
    "It is based on Selenium and ChromeDriver to scrape web pages on Google Chrome, on a Windows Machine.\n",
    "\n",
    "The scraper reads text from a table in https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_area\n",
    "opens a new tab and downloads files from https://silhouettegarden.com/category/country/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b8ffa6",
   "metadata": {},
   "source": [
    "### Set-up Instructions:\n",
    "\n",
    "Set up your directory to contain:\n",
    "- src/scraper.py\n",
    "- chromedriver_win32/chromedriver.exe\n",
    "\n",
    "You should first download chromedriver.exe from https://chromedriver.chromium.org/downloads "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc456f",
   "metadata": {},
   "source": [
    "### Code:\n",
    "\n",
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce946d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import ast\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81fd23f",
   "metadata": {},
   "source": [
    "Intialize the scraper and navigate to desired URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ece2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open Chrome and navigate to desired web page\n",
    "\n",
    "chromedriver_folder = \"../chromedriver_99_win32/chromedriver\"\n",
    "URL = \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_area\"\n",
    "op = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(chromedriver_folder, options=op)\n",
    "driver.get(URL)\n",
    "\n",
    "#Try opening the webpage\n",
    "Table_xpath = '/html/body/div[3]/div[3]/div[5]/div[1]/table[2]'\n",
    "try:\n",
    "    #Here we check if the table we want to read is shown and wait until this happens\n",
    "    table_ok = EC.presence_of_element_located((By.XPATH,Table_xpath ))\n",
    "    WebDriverWait(driver, 5).until(table_ok)\n",
    "    time.sleep(2)\n",
    "except TimeoutException:\n",
    "    print(\"Timed out waiting for page to load\")\n",
    "\n",
    "    \n",
    "table = driver.find_element_by_xpath(Table_xpath)\n",
    "rows = table.find_elements_by_tag_name(\"tr\") #this line identifies all the rows in the table\n",
    "\n",
    "#Initialize data storage\n",
    "Areas = []\n",
    "Countries = []\n",
    "\n",
    "#Loop through the rows to get data\n",
    "for i in range(1,len(rows)):\n",
    "    r = rows[i]\n",
    "    values = r.find_elements_by_tag_name(\"td\")\n",
    "    n_cols = len(values)\n",
    "    if n_cols == 7:\n",
    "        country = values[1].text\n",
    "        areas = (values[2].text).split()\n",
    "    else:\n",
    "        country = values[0].text\n",
    "        areas = (values[1].text).split()\n",
    "\n",
    "    try:\n",
    "        area = areas[0]\n",
    "        area = float(area.replace(',',''))\n",
    "        Areas.append(area)\n",
    "        Countries.append(country)\n",
    "        print(\"Country \",country,\" Area \",area)\n",
    "    except:\n",
    "        print(\"no data\")\n",
    "\n",
    "#Create Pandas dataframe and save it\n",
    "df = pd.DataFrame({'Countries': Countries,'Areas (Km2)': Areas})\n",
    "df.to_csv(\"WorldData.csv\", index=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0d551",
   "metadata": {},
   "source": [
    "Open a new tab to run the scraper to download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80bc13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://silhouettegarden.com/category/country/\"\n",
    "driver.execute_script('''window.open(URL,\"_blank\");''')\n",
    "\n",
    "Data_element = '/html/body/div[1]/div[3]/div/div[2]'\n",
    "try:\n",
    "    ok = EC.presence_of_element_located((By.XPATH, Data_element))\n",
    "    WebDriverWait(driver, 5).until(ok)\n",
    "    time.sleep(2)\n",
    "except TimeoutException:\n",
    "    print(\"Timed out waiting for page to load\")\n",
    "    \n",
    "#get the opened tabs\n",
    "handles = driver.window_handles\n",
    "\n",
    "#go to new tab\n",
    "driver.switch_to.window(handles[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8aa9f",
   "metadata": {},
   "source": [
    "When downloading files from https://silhouettegarden.com/category/country/ the craper needs to click on the icon. This redirects to a new page and then press the download button.\n",
    "To download additional file it needs to go back to the previous page, but this causes a loss of data structures in selenium.\n",
    "\n",
    "In order to overocme this, a recursive approach needs to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c971b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the recursive download function\n",
    "# It keeps downloading and reupdating selenium structure untill all elements have been visited\n",
    "def DownloadFiles(driver,idx_row,idx_country):\n",
    "    ok = EC.presence_of_element_located((By.XPATH, '/html/body/div[1]/div[3]/div/div[2]'))\n",
    "    WebDriverWait(driver, 1).until(ok)\n",
    "    \n",
    "    #This lines re-upload selnium structure by pointing to the part of the web page containing the counties and identifying each row\n",
    "    Full_images = driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div/div[2]\")\n",
    "    rows = Full_images.find_elements_by_xpath(\"//div[contains(@class, 'margin-bottom-30')]\")\n",
    "\n",
    "    N_rows = len(rows)\n",
    "\n",
    "    d = False\n",
    "    \n",
    "    #if all rows in web page have been visited, stop recursion\n",
    "    if idx_row == N_rows:\n",
    "        return True\n",
    "    \n",
    "    #each row has a certain amount of countries\n",
    "    r = rows[idx_row]\n",
    "    countries = r.find_elements_by_tag_name(\"div\")\n",
    "    N_countries = len(countries)\n",
    "    \n",
    "    #click on the cuntry to be redirected to downloading page\n",
    "    c = countries[idx_country]\n",
    "    c.click()\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    ok = EC.presence_of_element_located((By.XPATH, '/html/body/div[1]/div[3]/div/div[2]/div[2]/div[2]/div[1]/div/div/div/div[1]'))\n",
    "    WebDriverWait(driver, 1).until(ok)\n",
    "    \n",
    "    #Find the download button and click it\n",
    "    download_button = driver.find_element_by_xpath(\n",
    "        \"/html/body/div[1]/div[3]/div/div[2]/div[2]/div[2]/div[1]/div/div/div/div[1]\")\n",
    "    download_button.click()\n",
    "    \n",
    "    # go back previous page\n",
    "    driver.back()\n",
    "    idx_country = idx_country + 1\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    #if all countries in the row have been visited, move to next row\n",
    "    if idx_country == N_countries:\n",
    "        idx_row = idx_row+1\n",
    "        idx_country = 0\n",
    "\n",
    "    d = DownloadFiles(driver,idx_row, idx_country)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029743b4",
   "metadata": {},
   "source": [
    "The web page has an element to switch from one page to another. We need to go through each wbpage to download all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b70693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep looping through all the webpages up to the final one\n",
    "NextPage_Available = True\n",
    "while(NextPage_Available):\n",
    "    #download images\n",
    "    Downloaded = False\n",
    "    while not Downloaded:\n",
    "        idx_row = 0\n",
    "        idx_country = 0\n",
    "        \n",
    "        #recursively download files and re-upload the web structure\n",
    "        Downloaded = DownloadFiles(driver,idx_row, idx_country)\n",
    "        print(\"\")\n",
    "        \n",
    "    # If the files in the current page have been downloaded, go to next page\n",
    "    Next_page = driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div/div[2]/div[1]/div/div\") #element to switch to new page\n",
    "    s = Next_page.find_elements_by_tag_name(\"li\") #this is a list of pages \n",
    "    b = s[-1] #this is the next page button\n",
    "    page_name = b.text\n",
    "    if page_name == \"Next Page\":\n",
    "        button = b.find_element_by_tag_name(\"i\")\n",
    "        button.click()\n",
    "        time.sleep(0.1)\n",
    "    else:\n",
    "        #if the \"Next Page\" button does not exist, stop the search. We reached the end\n",
    "        NextPage_Available = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
