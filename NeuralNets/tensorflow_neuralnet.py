# -*- coding: utf-8 -*-
"""Tensorflow_NeuralNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x4_eHXM2-WfE_QhRjvnUibPJOZypxddb
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/Colab Notebooks/Classification_ML/"

import tensorflow as tf
import tensorflow_probability as tfp
import tensorflow_datasets.public_api as tfds

import cv2 as cv
from google.colab.patches import cv2_imshow
import numpy as np
import matplotlib.pyplot as plt
import random

#Load MNIST data
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

img = x_train[0]
img = img.reshape((img.shape[0],img.shape[1],1))
img = img[:,:,0] #for gray scale only "D array needed"
plt.imshow(img,cmap='gray')
print("Value",y_train[0] )

input_dim = img.shape[0]*img.shape[1]
output_dim = 10

class MLP(tf.keras.Sequential):
    def __init__(self, input_dim,h_sizes, output_dim,activation):
        super(MLP, self).__init__()

        self.add(tf.keras.layers.Flatten()) #flattens the image size
        input_layer = tf.keras.layers.Input(shape=input_dim)  # instantiate Keras input tensor
        self.add(input_layer)

        for k in range(len(h_sizes)):
            layer = tf.keras.layers.Dense(h_sizes[k], activation=activation)
            self.add(layer)

        self.add(tf.keras.layers.Dense(output_dim,activation=tf.nn.softmax)) #ouptu layer

        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
        self.optimizer = tf.keras.optimizers.Adam(lr=0.001)


        self.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(lr=0.001),
              metrics=['accuracy'])
        
        #for one-hot-encoed labels
        # self.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
        #       optimizer=tf.keras.optimizers.Adam(),
        #       metrics=['accuracy'])

        self.input_dim = input_dim
        self.h_sizes = h_sizes
        self.output_dim = output_dim

    def train(self,X_train,Y_train,X_test,Y_test,batch_size,TrainType=0, epochs = 1e03, Err = 1e-06, dErr = 1e-07):

        if TrainType == 0:
          print("Fitting")
          self.fit(X_train,
        Y_train,
        batch_size=batch_size,
        epochs=epochs,
        verbose=1,
        validation_data=(X_test, Y_test))

        elif TrainType == 1:
          epoch_lim = epochs

          batch_loss = 1
          iter = 0

          n_batches = X_train.shape[0]//batch_size

          X_train_batch = tf.split(X_train, num_or_size_splits=n_batches, axis=0)
          Y_train_batch = tf.split(Y_train, num_or_size_splits=n_batches, axis=0)

          for epoch in range(epochs):

            epoch_loss = 0
            correct = 0

            for i in range(len(X_train_batch)):
              x = X_train_batch[i]
              y = Y_train_batch[i]

              batch_loss = self.train_on_batch(x, y)
              epoch_loss = epoch_loss+batch_loss[0]
              correct = correct+batch_loss[1]
              
            epoch_loss = epoch_loss/len(X_train_batch)
            correct = correct/len(X_train_batch)
            print("Epoch", epoch, " Loss ", epoch_loss," accuracy", correct)

        elif TrainType == 2:
          n_batches = X_train.shape[0]//batch_size

          X_train_batch = tf.split(X_train, num_or_size_splits=n_batches, axis=0)
          Y_train_batch = tf.split(Y_train, num_or_size_splits=n_batches, axis=0)

          for epoch in range(epochs):
            correct = 0
            i = 0
            loss_epoch = 0
            for i in range(len(X_train_batch)):
              x = X_train_batch[i]
              y = Y_train_batch[i]

              with tf.GradientTape() as tape:

                y_pred = self.call(x)            
                loss_value = self.loss(y,y_pred)

                grads = tape.gradient(loss_value, self.trainable_variables)
                self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

                class_pred = np.argmax(y_pred.numpy(),axis=1)
                correct = correct+(y.numpy()==class_pred).sum()/y.shape[0]

                loss_epoch = loss_epoch+loss_value.numpy()

            epoch_accuracy = correct/n_batches
            epoch_loss = loss_epoch/n_batches

            #test set
            y_pred = self.predict(X_test)
            loss_value = self.loss(Y_test,y_pred).numpy()
            class_pred = np.argmax(y_pred,axis=1)
            correct = (Y_test==class_pred).sum()/Y_test.shape[0]
            
            print("***epoch: ",epoch," train loss epoch:",epoch_loss," train accuracy:",epoch_accuracy)
            print("                    test loss:",loss_value," test accuracy:",correct)



class MLP_2():
  def __init__(self, input_dim,h_sizes, output_dim,activation):
        super(MLP_2, self).__init__()

        input = tf.keras.layers.Input(shape=(input_dim[0],input_dim[1]))
        x = tf.keras.layers.Flatten()(input)

        for k in range(len(h_sizes)):
            x = tf.keras.layers.Dense(h_sizes[k], activation=activation)(x)

        output = tf.keras.layers.Dense(output_dim,activation=tf.nn.softmax)(x) #ouptu layer

        self.model = tf.keras.Model(inputs=input,outputs=output)

        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
        self.optimizer = tf.keras.optimizers.Adam(lr=0.001)

        self.model.compile(loss=self.loss,
              optimizer=self.optimizer,
              metrics=['accuracy'])

  def train(self,X_train,Y_train,X_test,Y_test,batch_size,TrainType=0, epochs = 1e03, Err = 1e-06, dErr = 1e-07):

      if TrainType == 0:
        print("Fitting")
        self.model.fit(X_train,
      Y_train,
      batch_size=batch_size,
      epochs=epochs,
      verbose=1,
      validation_data=(X_test, Y_test))

      elif TrainType == 1:
        epoch_lim = epochs

        batch_loss = 1
        iter = 0

        n_batches = X_train.shape[0]//batch_size

        X_train_batch = tf.split(X_train, num_or_size_splits=n_batches, axis=0)
        Y_train_batch = tf.split(Y_train, num_or_size_splits=n_batches, axis=0)

        for epoch in range(epochs):

          epoch_loss = 0
          correct = 0

          for i in range(len(X_train_batch)):
            x = X_train_batch[i]
            y = Y_train_batch[i]

            batch_loss = self.model.train_on_batch(x, y)
            epoch_loss = epoch_loss+batch_loss[0]
            correct = correct+batch_loss[1]
            
          epoch_loss = epoch_loss/len(X_train_batch)
          correct = correct/len(X_train_batch)
          print("Epoch", epoch, " Loss ", epoch_loss," accuracy", correct)

      elif TrainType == 2:
        n_batches = X_train.shape[0]//batch_size

        X_train_batch = tf.split(X_train, num_or_size_splits=n_batches, axis=0)
        Y_train_batch = tf.split(Y_train, num_or_size_splits=n_batches, axis=0)

        for epoch in range(epochs):
          correct = 0
          i = 0
          loss_epoch = 0
          for i in range(len(X_train_batch)):
            x = X_train_batch[i]
            y = Y_train_batch[i]

            with tf.GradientTape() as tape:

              y_pred = self.model.call(x)            
              loss_value = self.loss(y,y_pred)

              grads = tape.gradient(loss_value, self.model.trainable_variables)
              self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))

              class_pred = np.argmax(y_pred.numpy(),axis=1)
              correct = correct+(y.numpy()==class_pred).sum()/y.shape[0]

              loss_epoch = loss_epoch+loss_value.numpy()

          epoch_accuracy = correct/n_batches
          epoch_loss = loss_epoch/n_batches

          #test set
          y_pred = self.model.predict(X_test)
          loss_value = self.loss(Y_test,y_pred).numpy()
          class_pred = np.argmax(y_pred,axis=1)
          correct = (Y_test==class_pred).sum()/Y_test.shape[0]
          
          print("***epoch: ",epoch," train loss epoch:",epoch_loss," train accuracy:",epoch_accuracy)
          print("                    test loss:",loss_value," test accuracy:",correct)

# Net = MLP(input_dim,[30], output_dim,"relu")
Net = MLP_2((img.shape[0],img.shape[1]),[30], output_dim,"relu")

Net.train(x_train,y_train,x_test,y_test,100,TrainType=0, epochs = 100, Err = 1e-06, dErr = 1e-07)

y_pred = Net.predict(x_test)
# y_pred = Net.model.predict(x_test)

#test on some images
randomlist = random.sample(range(10, 30), 5)

for n in randomlist:
  img = x_test[n,:,:]
  img = img.reshape((img.shape[0],img.shape[1],1))
  img = img[:,:,0] #for gray scale only "D array needed"
  plt.imshow(img,cmap='gray')
  plt.show()
  print("Value",y_test[n]," predicted",np.argmax(y_pred[n]) )